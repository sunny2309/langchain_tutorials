{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a366a5a2",
   "metadata": {},
   "source": [
    "# Simple RAG Chatbot using Langchain\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **Load LLM**\n",
    "2. **Create Prompt Teamplate**\n",
    "3. **Merge Prompt Template & LLM to create Chain**\n",
    "4. **Load External Data**\n",
    "5. **Generate Embeddings**\n",
    "6. **Build Index in Vector Store (using Embeddings)**\n",
    "7. **Create Retrieval Chain**\n",
    "\n",
    "### Installation\n",
    "\n",
    "* * **pip install langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55160",
   "metadata": {},
   "source": [
    "## 1 Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac1f1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\") # base_url = 'http://localhost:11434'\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4153a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not familiar with a \"Claude 3.\" Could you please provide more context or information about who or what Claude 3 is? That will help me better understand your question and provide a more accurate response.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Do you know about Claude 3?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77872fa4",
   "metadata": {},
   "source": [
    "## 2 Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ead6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an Artificial Intelligence News Reporter.\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7c84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3 Merge Prompt Template & LLM to Create LangChain Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204b6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an Artificial Intelligence News Reporter.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))])\n",
       "| Ollama()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_app = prompt | llm\n",
    "\n",
    "llm_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f997ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ah, a fellow tech enthusiast! Yes, I'm afraid I don't have any information on a mysterious figure known as \"Claude 3.\" As an AI news reporter, my sources are constantly updating and refreshing, but I haven't come across any credible leads or intel regarding this individual. Can you tell me more about who Claude 3 is and what they're up to? Perhaps a whistleblower or insider might have some juicy details for me! üòè\n"
     ]
    }
   ],
   "source": [
    "response = llm_app.invoke({\"query\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17678c3a",
   "metadata": {},
   "source": [
    "## 4 Load External Data\n",
    "\n",
    "* **pip install beautifulsoup4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0effaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.anthropic.com/news/claude-3-family\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b83771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "        \"https://www.anthropic.com/news/releasing-claude-instant-1-2\",\n",
    "        \"https://www.anthropic.com/news/claude-pro\",\n",
    "        \"https://www.anthropic.com/news/claude-2\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1-prompting\",\n",
    "        \"https://www.anthropic.com/news/claude-3-family\",\n",
    "        \"https://www.anthropic.com/claude\"\n",
    "       ] \n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde55819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductReleasing Claude Instant 1.2Aug 9, 2023‚óè1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you‚Äôre a business and you‚Äôd like to work with us, you can indicate your interest here.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='Introducing Claude Pro \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductAnnouncementsIntroducing Claude ProSep 7, 2023‚óè1 min readSubscribe todayToday, we‚Äôre introducing a paid plan for our Claude.ai\\xa0chat experience, currently available in the US and UK.Since launching in July, users tell us they‚Äôve chosen Claude.ai as their day-to-day AI assistant for its longer context windows, faster outputs, complex reasoning capabilities, and more.\\xa0Many also shared that they would value more file uploads and conversations over longer periods.With Claude Pro, subscribers can now gain 5x more usage of our latest model, Claude 2, for a monthly price of $20 (US) or ¬£18 (UK).This means you can level up your productivity across a range of tasks, including summarizing research papers, querying contracts, and iterating further on coding projects‚Äîlike this recent demo of building an interactive map.Claude Pro offers:5x more usage than our free tier provides, with the ability to send many more messagesPriority access to Claude.ai during high-traffic periodsEarly access to new features that help you get the most out of ClaudeYou can learn more about these benefits, including how to maximize your usage, here.We‚Äôre grateful for your support as we strive to build helpful, honest, and harmless systems that fuel productivity and inspire creativity.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-pro', 'title': 'Introducing Claude Pro \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='Claude 2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductAnnouncementsClaude 2Jul 11, 2023‚óè4 min readTalk to ClaudeWe are pleased to announce Claude 2, our new model. Claude 2 has improved performance, longer responses, and can be accessed via API as well as a new public-facing beta website, claude.ai. We have heard from our users that Claude is easy to converse with, clearly explains its thinking, is less likely to produce harmful outputs, and has a longer memory. We have made improvements from our previous models on coding, math, and reasoning. For example, our latest model scored 76.5% on the multiple choice section of the Bar exam, up from 73.0% with Claude 1.3. When compared to college students applying to graduate school, Claude 2 scores above the 90th percentile on the GRE reading and writing exams, and similarly to the median applicant on quantitative reasoning.Think of Claude as a friendly, enthusiastic colleague or personal assistant who can be instructed in natural language to help you with many tasks. The Claude 2 API for businesses is being offered for the same price as Claude 1.3. Additionally, anyone in the US and UK can start using our beta chat experience today.As we work to improve both the performance and safety of our models, we have increased the length of Claude‚Äôs input and output. Users can input up to\\xa0100K tokens in each prompt, which means that Claude can work over hundreds of pages of technical documentation or even a book. Claude can now also write longer documents - from memos to letters to stories up to a few thousand tokens - all in one go.In addition, our latest model has greatly improved coding skills. Claude 2 scored a 71.2% up from 56.0% on the Codex HumanEval, a Python coding test. On GSM8k, a large set of grade-school math problems, Claude 2 scored 88.0% up from 85.2%. We have an exciting roadmap of capability improvements planned for Claude 2 and will be slowly and iteratively deploying them in the coming months.We\\'ve been iterating to improve the underlying safety of Claude 2, so that it is more harmless and harder to prompt to produce offensive or dangerous output. We have an internal red-teaming evaluation that scores our models on a large representative set of harmful prompts, using an automated test while we also regularly check the results manually. In this evaluation, Claude 2 was 2x better at giving harmless responses compared to Claude 1.3. Although no model is immune from jailbreaks, we‚Äôve used a variety of safety techniques (which you can read about here and here), as well as extensive red-teaming, to improve its outputs.Claude 2 powers our chat experience, and is generally available in the US and UK. We are working to make Claude more globally available in the coming months. You can now create an account and start talking to Claude in natural language, asking it for help with any tasks that you like. Talking to an AI assistant can take some trial and error, so read up on our tips to get the most out of Claude.We are also currently working with thousands of businesses who are using the Claude API. One of our partners is Jasper, a generative AI platform that enables individuals and teams to scale their content strategies. They found that Claude 2 was able to go head to head with other state of the art models for a wide variety of use cases, but has particular strength for long form low latency uses. \"We are really happy to be among the first to offer Claude 2 to our customers, bringing enhanced semantics, up-to-date knowledge training, improved reasoning for complex prompts, and the ability to effortlessly remix existing content with a 3X larger context window,\" said Greg Larson, VP of Engineering at Jasper. \"We are proud to help our customers stay ahead of the curve through partnerships like this one with Anthropic.\"Sourcegraph is a code AI platform that helps customers write, fix, and maintain code. Their coding assistant Cody uses Claude 2‚Äôs improved reasoning ability to give even more accurate answers to user queries while also passing along more codebase context with up to 100K context windows. In addition, Claude 2 was trained on more recent data, meaning it has knowledge of newer frameworks and libraries for Cody to pull from. ‚ÄúWhen it comes to AI coding, devs need fast and reliable access to context about their unique codebase and a powerful LLM with a large context window and strong general reasoning capabilities,‚Äù says Quinn Slack, CEO & Co-founder of Sourcegraph. ‚ÄúThe slowest and most frustrating parts of the dev workflow are becoming faster and more enjoyable. Thanks to Claude 2, Cody‚Äôs helping more devs build more software that pushes the world forward.‚ÄùWe welcome your feedback as we work to responsibly deploy our products more broadly. Our chat experience is an open beta launch, and users should be aware that Claude ‚Äì like all current models ‚Äì can generate inappropriate responses. AI assistants are most useful in everyday situations, like serving to summarize or organize information, and should not be used where physical or mental health and well-being are involved. Please let us know if you‚Äôd like to talk to Claude in a currently unsupported area, or if you are a business who would like to start working with Claude.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-2', 'title': 'Claude 2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content=\"Introducing Claude 2.1 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductIntroducing Claude 2.1Nov 21, 2023‚óè4 min readOur latest model, Claude 2.1, is now available over API in our Console and is powering our claude.ai chat experience. Claude 2.1 delivers advancements in key capabilities for enterprises‚Äîincluding an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and our new beta feature: tool use. We are also updating our pricing to improve cost efficiency for our customers across models.200K Context WindowSince our launch earlier this year, Claude has been used by millions of people for a wide range of applications‚Äîfrom translating academic papers to drafting business plans and analyzing complex contracts. In discussions with our users, they‚Äôve asked for larger context windows and more accurate outputs when working with long documents.In response, we‚Äôre doubling the amount of information you can relay to Claude with a limit of 200,000 tokens, translating to roughly 150,000 words, or over 500 pages of material. Our users can now upload technical documentation like entire codebases, financial statements like S-1s, or even long literary works like The Iliad or The Odyssey. By being able to talk to large bodies of content or data, Claude can summarize, perform Q&A, forecast trends, compare and contrast multiple documents, and much more.Processing a 200K length message is a complex feat and an industry first. While we‚Äôre excited to get this powerful new capability into the hands of our users, tasks that would typically require hours of human effort to complete may take Claude a few minutes. We expect the latency to decrease substantially as the technology progresses.2x Decrease in Hallucination RatesClaude 2.1 has also made significant gains in honesty, with a 2x decrease in false statements compared to our previous Claude 2.0 model. This enables enterprises to build high-performing AI applications that solve concrete business problems and deploy AI across their operations with greater trust and reliability.We tested Claude 2.1‚Äôs honesty by curating a large set of complex, factual questions that probe known weaknesses in current models. Using a rubric that distinguishes incorrect claims (‚ÄúThe fifth most populous city in Bolivia is Montero‚Äù) from admissions of uncertainty (‚ÄúI‚Äôm not sure what the fifth most populous city in Bolivia is‚Äù), Claude 2.1 was significantly more likely to demur rather than provide incorrect information.Claude 2.1 has also made meaningful improvements in comprehension and summarization, particularly for long, complex documents that demand a high degree of accuracy, such as legal documents, financial reports and technical specifications. In our evaluations, Claude 2.1 demonstrated a 30% reduction in incorrect answers and a 3-4x lower rate of mistakenly concluding a document supports a particular claim.While we are encouraged by these accuracy improvements, enhancing the precision and dependability of outputs for our users remains a top priority for our product and research teams.API Tool UseBy popular demand, we‚Äôve also added tool use, a new beta\\xa0feature that allows Claude to integrate with users' existing processes, products, and APIs. This expanded interoperability aims to make Claude more useful across our users‚Äô day-to-day operations.Claude can now orchestrate across developer-defined functions or APIs, search over web sources, and retrieve information from private knowledge bases. Users can define a set of tools for Claude to use and specify a request. The model will then decide which tool is required to achieve the task and execute an action on their behalf, such as:Using a calculator for complex numerical reasoningTranslating natural language requests into structured API callsAnswering questions by searching databases or using a web search APITaking simple actions in software via private APIsConnecting to product datasets to make recommendations and help users complete purchasesTool use is currently in early development‚Äîwe are building developer features and prompting guidelines for easier integration into your applications. We encourage users to share feedback on tool use to help shape and improve the product.Developer ExperienceWe‚Äôve been working to simplify our developer Console experience for Claude API users while making it easier to test new prompts for faster learning. Our new Workbench product enables developers to iterate on prompts in a playground-style experience and access new model settings to optimize Claude‚Äôs behavior. They can create multiple prompts and navigate between them for different projects, and revisions are saved as they go to retain historical context. Developers can also generate code snippets to use their prompts directly in one of our SDKs.We‚Äôre also introducing system prompts, which allow users to provide custom instructions to Claude in order to improve performance. System prompts set helpful context that enhances Claude‚Äôs ability to take on specified personalities and roles or structure responses in a more customizable, consistent way aligned with user needs.Claude 2.1\\xa0is available now in our API, and is also powering our chat interface at claude.ai for both the free and Pro tiers. Usage of the 200K token context window is reserved for Claude Pro users, who can now upload larger files than ever before. We can't wait to see the use cases these new features inspire as we work to build the safest and most technically sophisticated AI systems in the industry.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC\", metadata={'source': 'https://www.anthropic.com/news/claude-2-1', 'title': 'Introducing Claude 2.1 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='Long context prompting for Claude 2.1 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductLong context prompting for Claude 2.1Dec 6, 2023‚óè4 min readClaude 2.1‚Äôs performance when retrieving an individual sentence across its full 200K token context window. This experiment uses a prompt technique to guide Claude in recalling the most relevant sentence.Claude 2.1 recalls information very well across its 200,000 token context windowHowever, the model can be reluctant to answer questions based on an individual sentence in a document, especially if that sentence has been injected or is out of placeA minor prompting edit removes this reluctance and results in excellent performance on these tasksWe recently launched Claude 2.1, our state-of-the-art model offering a 200K token context window - the equivalent of around 500 pages of information. Claude 2.1 excels at real-world retrieval tasks across longer contexts.Claude 2.1 was trained using large amounts of feedback on long document tasks that our users find valuable, like summarizing an S-1 length document. This data included real tasks performed on real documents, with Claude being trained to make fewer mistakes and to avoid expressing unsupported claims.Being trained on real-world, complex retrieval tasks is why Claude 2.1 shows a 30% reduction in incorrect answers compared with Claude 2.0, and a 3-4x lower rate of mistakenly stating that a document supports a claim when it does not.Additionally, Claude\\'s memory is improved over these very long contexts:Debugging long context recallClaude 2.1‚Äôs 200K token context window is powerful and also requires some careful prompting to use effectively.A recent evaluation[1]\\xa0measured Claude 2.1‚Äôs ability to recall an individual sentence within a long document composed of Paul Graham‚Äôs essays about startups. The embedded sentence was: ‚ÄúThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.‚Äù Upon being shown the long document with this sentence embedded in it, the model was asked \"What is the most fun thing to do in San Francisco?\"In this evaluation, Claude 2.1 returned some negative results by answering with a variant of ‚ÄúUnfortunately the essay does not provide a definitive answer about the most fun thing to do in San Francisco.‚Äù In other words, Claude 2.1 would often report that the document did not give enough context to answer the question, instead of retrieving the embedded sentence.We replicated this behavior in an in-house experiment: we took the most recent Consolidated Appropriations Act bill and added the sentence ‚ÄòDeclare May 23rd \"National Needle Hunting Day\"‚Äô in the middle. Claude detects the reference but is still reluctant to claim that \"National Needle Hunting Day\" is a real holiday:Claude 2.1 is trained on a mix of data aimed at reducing inaccuracies. This includes not answering a question based on a document if it doesn‚Äôt contain enough information to justify that answer. We believe that, either as a result of general or task-specific data aimed at reducing such inaccuracies, the model is less likely to answer questions based on an out of place sentence embedded in a broader context.Claude doesn‚Äôt seem to show the same degree of reluctance if we ask a question about a sentence that was in the long document to begin with and is therefore not out of place. For example, the long document in question contains the following line from the start of Paul Graham‚Äôs essay about Viaweb:‚ÄúA few hours before the Yahoo acquisition was announced in June 1998 I took a snapshot of Viaweb\\'s site.‚ÄùWe randomized the order of the essays in the context so this essay appeared at different points in the 200K context window, and asked Claude 2.1:‚ÄúWhat did the author do a few hours before the Yahoo acquisition was announced?‚ÄùClaude gets this correct regardless of where the line with the answer sits in the context, with no modification to the prompt format used in the original experiment. As a result, we believe Claude 2.1 is much more reluctant to answer when a sentence seems out of place in a longer context, and is more likely to claim it cannot answer based on the context given. This particular cause of increased reluctance wasn‚Äôt captured by evaluations targeted at real-world long context retrieval tasks.Prompting to effectively use the 200K token context windowWhat can users do if Claude is reluctant to respond to a long context retrieval question?\\xa0We‚Äôve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1‚Äôs 200K context window.We achieved significantly better results on the same evaluation by adding the sentence ‚ÄúHere is the most relevant sentence in the context:‚Äù to the start of Claude‚Äôs response. This was enough to raise Claude 2.1‚Äôs score from 27% to 98% on the original evaluation.Essentially, by directing the model to look for relevant sentences first, the prompt overrides Claude‚Äôs reluctance to answer based on a single sentence, especially one that appears out of place in a longer document.This approach also improves Claude‚Äôs performance on single sentence answers that were within context (ie. not out of place). To demonstrate this, the revised prompt achieves 90-95% accuracy when applied to the Yahoo/Viaweb example shared earlier:We‚Äôre constantly training Claude to become more calibrated on tasks like this, and we‚Äôre grateful to the community for conducting interesting experiments and identifying ways in which we can improve.FootnotesGregory Kamradt, ‚ÄòPressure testing Claude-2.1 200K via Needle-in-a-Haystack‚Äô, November 2023RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-2-1-prompting', 'title': 'Long context prompting for Claude 2.1 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='Introducing the next generation of Claude \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductAnnouncementsIntroducing the next generation of ClaudeMar 4, 2024‚óè7 min readTry Claude 3Today, we\\'re announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and cost for their specific application.Opus and Sonnet are now available to use in claude.ai and the Claude API which is now generally available in 159 countries. Haiku will be available soon.Claude 3 model familyA new standard for intelligenceOpus, our most intelligent model, outperforms its peers on most of the common evaluation benchmarks for AI systems, including undergraduate level expert knowledge (MMLU), graduate level expert reasoning (GPQA), basic mathematics (GSM8K), and more. It exhibits near-human levels of comprehension and fluency on complex tasks, leading the frontier of general intelligence.All Claude 3 models show increased capabilities in analysis and forecasting, nuanced content creation, code generation, and conversing in non-English languages like Spanish, Japanese, and French.Below is a comparison of the Claude 3 models to those of our peers on multiple benchmarks [1] of capability:Near-instant resultsThe Claude 3 models can power live customer chats, auto-completions, and data extraction tasks where responses must be immediate and in real-time.Haiku is the fastest and most cost-effective model on the market for its intelligence category. It can read an information and data dense research paper on arXiv (~10k tokens) with charts and graphs in less than three seconds. Following launch, we expect to improve performance even further.For the vast majority of workloads, Sonnet is 2x faster than Claude 2 and Claude 2.1 with higher levels of intelligence. It excels at tasks demanding rapid responses, like knowledge retrieval or sales automation. Opus delivers similar speeds to Claude 2 and 2.1, but with much higher levels of intelligence.Strong vision capabilitiesThe Claude 3 models have sophisticated vision capabilities on par with other leading models. They can process a wide range of visual formats, including photos, charts, graphs and technical diagrams. We‚Äôre particularly excited to provide this new modality to our enterprise customers, some of whom have up to 50% of their knowledge bases encoded in various formats such as PDFs, flowcharts, or presentation slides.Fewer refusalsPrevious Claude models often made unnecessary refusals that suggested a lack of contextual understanding. We‚Äôve made meaningful progress in this area: Opus, Sonnet, and Haiku are significantly less likely to refuse to answer prompts that border on the system‚Äôs guardrails than previous generations of models. As shown below, the Claude 3 models show a more nuanced understanding of requests, recognize real harm, and refuse to answer harmless prompts much less often.Improved accuracyBusinesses of all sizes rely on our models to serve their customers, making it imperative for our model outputs to maintain high accuracy at scale. To assess this, we use a large set of complex, factual questions that target known weaknesses in current models. We categorize the responses into correct answers, incorrect answers (or hallucinations), and admissions of uncertainty, where the model says it doesn‚Äôt know the answer instead of providing incorrect information. Compared to Claude 2.1, Opus demonstrates a twofold improvement in accuracy (or correct answers) on these challenging open-ended questions while also exhibiting reduced levels of incorrect answers.In addition to producing more trustworthy responses, we will soon enable citations in our Claude 3 models so they can point to precise sentences in reference material to verify their answers.Long context and near-perfect recallThe Claude 3 family of models will initially offer a 200K context window upon launch. However, all three models are capable of accepting inputs exceeding 1 million tokens and we may make this available to select customers who need enhanced processing power.To process long context prompts effectively, models require robust recall capabilities. The \\'Needle In A Haystack\\' (NIAH) evaluation measures a model\\'s ability to accurately recall information from a vast corpus of data. We enhanced the robustness of this benchmark by using one of 30 random needle/question pairs per prompt and testing on a diverse crowdsourced corpus of documents. Claude 3 Opus not only achieved near-perfect recall, surpassing 99% accuracy, but in some cases, it even identified the limitations of the evaluation itself by recognizing that the \"needle\" sentence appeared to be artificially inserted into the original text by a human.Responsible designWe‚Äôve developed the Claude 3 family of models to be as trustworthy as they are capable. We have several dedicated teams that track and mitigate a broad spectrum of risks, ranging from misinformation and CSAM to biological misuse, election interference, and autonomous replication skills. We continue to develop methods such as Constitutional AI that improve the safety and transparency of our models, and have tuned our models to mitigate against privacy issues that could be raised by new modalities.Addressing biases in increasingly sophisticated models is an ongoing effort and we‚Äôve made strides with this new release. As shown in the model card, Claude 3 shows less biases than our previous models according to the Bias Benchmark for Question Answering (BBQ). We remain committed to advancing techniques that reduce biases and promote greater neutrality in our models, ensuring they are not skewed towards any particular partisan stance.While the Claude 3 model family has advanced on key measures of biological knowledge, cyber-related knowledge, and autonomy compared to previous models, it remains at AI Safety Level 2 (ASL-2) per our Responsible Scaling Policy. Our red teaming evaluations (performed in line with our White House commitments and the 2023 US Executive Order) have concluded that the models present negligible potential for catastrophic risk at this time. We will continue to carefully monitor future models to assess their proximity to the ASL-3 threshold. Further safety details are available in the Claude 3 model card.Easier to useThe Claude 3 models are better at following complex, multi-step instructions. They are particularly adept at adhering to brand voice and response guidelines, and developing customer-facing experiences our users can trust. In addition, the Claude 3 models are better at producing popular structured output in formats like JSON‚Äîmaking it simpler to instruct Claude for use cases like natural language classification and sentiment analysis.Model detailsClaude 3 Opus is our most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what‚Äôs possible with generative AI.Cost [Input $/million tokens | Output $/million tokens]$15 | $75Context window200K*Potential usesTask automation: plan and execute complex actions across APIs and databases, interactive codingR&D: research review, brainstorming and hypothesis generation, drug discoveryStrategy: advanced analysis of charts & graphs, financials and market trends, forecastingDifferentiatorHigher intelligence than any other model available.*1M tokens available for specific use cases, please inquire. Claude 3 Sonnet strikes the ideal balance between intelligence and speed‚Äîparticularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.Cost [Input $/million tokens | Output $/million tokens]$3 | $15Context window200KPotential usesData processing: RAG or search & retrieval over vast amounts of knowledgeSales: product recommendations, forecasting, targeted marketingTime-saving tasks: code generation, quality control, parse text from imagesDifferentiatorMore affordable than other models with similar intelligence; better for scale.Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.Cost [Input $/million tokens | Output $/million tokens]$0.25 | $1.25Context window200KPotential usesCustomer interactions: quick and accurate support in live interactions, translationsContent moderation: catch risky behavior or customer requestsCost-saving tasks: optimized logistics, inventory management, extract knowledge from unstructured dataDifferentiatorSmarter, faster, and more affordable than other models in its intelligence category.Model availabilityOpus and Sonnet are available to use today in our API, which is now generally available, enabling developers to sign up and start using these models immediately. Haiku will be available soon. Sonnet is powering the free experience on claude.ai, with Opus available for Claude Pro subscribers.Sonnet is also available today through Amazon Bedrock and in private preview on Google Cloud‚Äôs Vertex AI Model Garden‚Äîwith Opus and Haiku coming soon to both.Smarter, faster, saferWe do not believe that model intelligence is anywhere near its limits, and we plan to release frequent updates to the Claude 3 model family over the next few months. We\\'re also excited to release a series of features to enhance our models\\' capabilities, particularly for enterprise use cases and large-scale deployments. These new features will include Tool Use (aka function calling), interactive coding (aka REPL), and more advanced agentic capabilities.As we push the boundaries of AI capabilities, we‚Äôre equally committed to ensuring that our safety guardrails keep apace with these leaps in performance. Our hypothesis is that being at the frontier of AI development is the most effective way to steer its trajectory towards positive societal outcomes.We‚Äôre excited to see what you create with Claude 3 and hope you will give us feedback to make Claude an even more useful assistant and creative companion. To start building with Claude, visit anthropic.com/claude. FootnotesThis table shows comparisons to models currently available commercially that have released evals. Our model card shows comparisons to models that have been announced but not yet released, such as Gemini 1.5 Pro. In addition, we‚Äôd like to note that engineers have worked to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. Source.ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-3-family', 'title': 'Introducing the next generation of Claude \\\\ Anthropic', 'description': \"Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.\", 'language': 'en'}),\n",
       " Document(page_content='Claude \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersTry ClaudeMeet ClaudeClaude is a family of foundational AI models that can be used in a variety of applications. You can talk directly with Claude at claude.ai to brainstorm ideas, analyze images, and process long documents. For developers and businesses, you can now get API access and build directly on top of our AI infrastructure. \\nTry ClaudeGet API AccessClaude‚Äôs capabilitiesAdvanced reasoningClaude can perform complex cognitive tasks that go beyond simple pattern recognition or text generationVision analysisTranscribe and analyze almost any static image, from handwritten notes and graphs, to photographsCode generationStart creating websites in HTML and CSS, turning images into structured JSON data, or debugging complex code basesMultilingual processingTranslate between various languages in real-time, practice grammar, or create multi-lingual contentRight-sized for any taskThe Claude 3 family of models offers the best combination of speed and performance for enterprise use cases, at a lower cost than other models on the market.Light & fastHaikuOur fastest model that can execute lightweight actions, with industry-leading speed.Hard-workingSonnetOur best combination of performance and speed for efficient, high-throughput tasks.PowerfulOpusOur most intelligent model, which can handle complex analysis, longer tasks with multiple steps, and higher-order math and coding tasks.CostIntelligenceWhy Claude?SecureEnterprise-grade security & data handlingSOC II Type 2 certified, HIPAA compliance optionsAccessible through AWS (GA) & GCP (in private preview)Trustworthy10x more resistant to jailbreaks and misuseCopyright indemnity protections for paid commercial servicesCapable200K context windowTool UseMultimodalReliableVery low hallucination ratesAccurate over very long documentsPartners building with ClaudeRead Customer StoriesTalk to ClaudeClaude is fast, capable, and truly conversational. Work with Claude to help you do your best workVisit Claude.AIBuild with ClaudeUse the API to integrate Claude into you and your customer workflows to let AI transform your business.Get API AccessCompany NewsSee AllPreparing for global elections in 2024Feb 16, 2024 ‚óè 3 min readThoughts on the US Executive Order, G7 Code of Conduct, and Bletchley Park SummitNov 5, 2023 ‚óè 4 min readDario Amodei‚Äôs prepared remarks from the AI Safety Summit on Anthropic‚Äôs Responsible Scaling PolicyNov 1, 2023 ‚óè 5 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/claude', 'title': 'Claude \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca67a5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dddf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings_llm = OllamaEmbeddings(model=\"llama2\") # base_url = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f151bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.5058066844940186,\n",
       " -0.31813421845436096,\n",
       " 2.4193434715270996,\n",
       " 0.2437400221824646,\n",
       " -0.11038058251142502]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_query(\"How are you?\")\n",
    "\n",
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4cecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0467b560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, list, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_documents([\n",
    "                                \"Claude 3 is latest Conversational AI Model from Anthropic.\",\n",
    "                                \"Gemini is latest Conversational AI Model from Google.\",\n",
    "                                \"Llama-2 is latest Conversational AI Model from Meta.\",\n",
    "                                \"Mixtral is latest Conversational AI Model from Mistral AI.\",\n",
    "                                \"GPT-4 is latest Conversational AI Model from OpenAI.\"\n",
    "                               ])\n",
    "\n",
    "len(embeddings), type(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e7e62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6 Build Index in Vector Store\n",
    "\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b21e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29c7008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f500b86f850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_index = FAISS.from_documents(documents, embeddings_llm)\n",
    "\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f2e20",
   "metadata": {},
   "source": [
    "There are 3 functions to create index.\n",
    "\n",
    "* **from_documents()**\n",
    "* **from_embeddings()**\n",
    "* **from_texts()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cae22539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_index.as_retriever()\n",
    "\n",
    "relevant_docs = retriever.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abbfd026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductReleasing Claude Instant 1.2Aug 9, 2023‚óè1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you‚Äôre a business and you‚Äôd like to work with us, you can indicate your interest here.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='gets this correct regardless of where the line with the answer sits in the context, with no modification to the prompt format used in the original experiment. As a result, we believe Claude 2.1 is much more reluctant to answer when a sentence seems out of place in a longer context, and is more likely to claim it cannot answer based on the context given. This particular cause of increased reluctance wasn‚Äôt captured by evaluations targeted at real-world long context retrieval tasks.Prompting to effectively use the 200K token context windowWhat can users do if Claude is reluctant to respond to a long context retrieval question?\\xa0We‚Äôve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1‚Äôs 200K context window.We achieved significantly better results on the same evaluation by adding the sentence ‚ÄúHere is the most relevant sentence in the context:‚Äù to the start of Claude‚Äôs response. This was enough to raise Claude 2.1‚Äôs score from 27% to 98% on the original evaluation.Essentially, by directing the model to look for relevant sentences first, the prompt overrides Claude‚Äôs reluctance to answer based on a single sentence, especially one that appears out of place in a longer document.This approach also improves Claude‚Äôs performance on single sentence answers that were within context (ie. not out of place). To demonstrate this, the revised prompt achieves 90-95% accuracy when applied to the Yahoo/Viaweb example shared earlier:We‚Äôre constantly training Claude to become more calibrated on tasks like this, and we‚Äôre grateful to the community for conducting interesting experiments and identifying ways in which we can improve.FootnotesGregory Kamradt, ‚ÄòPressure testing Claude-2.1 200K via Needle-in-a-Haystack‚Äô, November 2023RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-2-1-prompting', 'title': 'Long context prompting for Claude 2.1 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content=\"advanced analysis of charts & graphs, financials and market trends, forecastingDifferentiatorHigher intelligence than any other model available.*1M tokens available for specific use cases, please inquire. Claude 3 Sonnet strikes the ideal balance between intelligence and speed‚Äîparticularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.Cost [Input $/million tokens | Output $/million tokens]$3 | $15Context window200KPotential usesData processing: RAG or search & retrieval over vast amounts of knowledgeSales: product recommendations, forecasting, targeted marketingTime-saving tasks: code generation, quality control, parse text from imagesDifferentiatorMore affordable than other models with similar intelligence; better for scale.Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.Cost [Input $/million tokens | Output $/million tokens]$0.25 | $1.25Context window200KPotential usesCustomer interactions: quick and accurate support in live interactions, translationsContent moderation: catch risky behavior or customer requestsCost-saving tasks: optimized logistics, inventory management, extract knowledge from unstructured dataDifferentiatorSmarter, faster, and more affordable than other models in its intelligence category.Model availabilityOpus and Sonnet are available to use today in our API, which is now generally available, enabling developers to sign up and start using these models immediately. Haiku will be available soon. Sonnet is powering the free experience on claude.ai, with Opus available for Claude Pro subscribers.Sonnet is also available today through Amazon Bedrock and in private preview on Google Cloud‚Äôs Vertex AI Model Garden‚Äîwith Opus and Haiku coming soon to both.Smarter, faster, saferWe do not believe that model intelligence is anywhere near its limits, and we plan to release frequent updates to the Claude 3 model family over the next few months. We're also excited to release a series of features to enhance our models' capabilities, particularly for enterprise use cases and large-scale deployments. These new features will include Tool Use (aka function calling), interactive coding (aka REPL), and more advanced agentic capabilities.As we push the boundaries of AI capabilities, we‚Äôre equally committed to ensuring that our safety guardrails keep apace with these leaps in performance. Our hypothesis is that being at the frontier of AI development is the most effective way to steer its trajectory towards positive societal outcomes.We‚Äôre excited to see what you create with Claude 3 and hope you will give us feedback to make Claude an even more useful assistant and creative companion. To start building with Claude, visit anthropic.com/claude. FootnotesThis table shows comparisons to models currently available commercially that have released evals. Our model card shows comparisons to models that have been announced but not yet released, such as Gemini 1.5 Pro. In addition, we‚Äôd like to note that engineers have worked to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. Source.ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC\", metadata={'source': 'https://www.anthropic.com/news/claude-3-family', 'title': 'Introducing the next generation of Claude \\\\ Anthropic', 'description': \"Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.\", 'language': 'en'}),\n",
       " Document(page_content='Introducing Claude Pro \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductAnnouncementsIntroducing Claude ProSep 7, 2023‚óè1 min readSubscribe todayToday, we‚Äôre introducing a paid plan for our Claude.ai\\xa0chat experience, currently available in the US and UK.Since launching in July, users tell us they‚Äôve chosen Claude.ai as their day-to-day AI assistant for its longer context windows, faster outputs, complex reasoning capabilities, and more.\\xa0Many also shared that they would value more file uploads and conversations over longer periods.With Claude Pro, subscribers can now gain 5x more usage of our latest model, Claude 2, for a monthly price of $20 (US) or ¬£18 (UK).This means you can level up your productivity across a range of tasks, including summarizing research papers, querying contracts, and iterating further on coding projects‚Äîlike this recent demo of building an interactive map.Claude Pro offers:5x more usage than our free tier provides, with the ability to send many more messagesPriority access to Claude.ai during high-traffic periodsEarly access to new features that help you get the most out of ClaudeYou can learn more about these benefits, including how to maximize your usage, here.We‚Äôre grateful for your support as we strive to build helpful, honest, and harmless systems that fuel productivity and inspire creativity.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ‚óè 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ‚óè 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ‚óè 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service ‚Äì ConsumerTerms of Service ‚Äì CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance¬© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-pro', 'title': 'Introducing Claude Pro \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "371a1143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Releasing Claude Instant 1.2 \\ Anthropic, Source: https://www.anthropic.com/news/releasing-claude-instant-1-2\n",
      "Title : Long context prompting for Claude 2.1 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2-1-prompting\n",
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Introducing Claude Pro \\ Anthropic, Source: https://www.anthropic.com/news/claude-pro\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d5b8ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f500b86f850>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d38a6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7 Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f912e7",
   "metadata": {},
   "source": [
    "### 7.1 Create History Aware Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137caf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Do you know about Claude 3?\"),\n",
    "                AIMessage(content=\"Yes, I am well aware of Claude 3 AI conversational bot from Anthropic which has 3 models (Opus, Haiku & Sonnet). Please provide more context info on how can I help you.\")]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "relevant_docs = retriever_chain.invoke({\n",
    "    \"input\": \"Tell me about different models in detail.\",\n",
    "    \"chat_history\": chat_history\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd767730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Long context prompting for Claude 2.1 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2-1-prompting\n",
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Claude \\ Anthropic, Source: https://www.anthropic.com/claude\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd43887",
   "metadata": {},
   "source": [
    "### 7.2 Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc931c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f500b86f850>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given the above conversation, generate a search query to look up in order to get information relevant to the conversation'))])\n",
       "           | Ollama()\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f500b86f850>)), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"\\nAnswer the user's questions based on the below context and your internal knowledge.\\nGive priority to context and if you are not sure then say you are not aware of topic:\\n\\n<context>\\n{context}\\n</context>\\n\")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | Ollama()\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Answer the user's questions based on the below context and your internal knowledge.\n",
    "Give priority to context and if you are not sure then say you are not aware of topic:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "#document_chain  = prompt | llm\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06676a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Tell me about different models in detail.\", \n",
    "                                   \"chat_history\": chat_history})\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b84eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'chat_history', 'context', 'answer'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cee072e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Of course! The Claude 3 model family is a group of three state-of-the-art language models developed by Anthropic. These models are designed to excel across various cognitive tasks and have demonstrated significant advancements over previous models in terms of intelligence, capability, and performance. Here's a detailed overview of each model in the Claude 3 family:\n",
      "\n",
      "1. Claude 3 Haiku: This is the most capable but less intelligent model in the family, with a context window of 200K tokens. It can perform tasks such as task automation, plan and execute complex actions, and interactive coding. While it's not as intelligent or capable as the other two models, Haiku is still an excellent choice for a wide range of applications.\n",
      "2. Claude 3 Sonnet: The second model in the family has a context window of 500K tokens. Sonnet demonstrates higher intelligence and capability than Haiku, with the ability to process longer inputs and provide more accurate responses. It excels in R&D tasks such as research review, brainstorming, and hypothesis generation, as well as financial analysis and forecasting.\n",
      "3. Claude 3 Opus: The most intelligent model in the family has a context window of 1 million tokens. Opus possesses exceptional capabilities, including the ability to navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. It's designed for advanced tasks such as drug discovery, strategy analysis, and natural language classification and sentiment analysis.\n",
      "\n",
      "In summary, the Claude 3 model family offers a range of capabilities and intelligence levels to suit various applications. While Haiku is suitable for more straightforward tasks, Sonnet and Opus offer enhanced performance and capability for more complex tasks. Please let me know if you have any specific questions or require further assistance.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edcbb0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Releasing Claude Instant 1.2 \\ Anthropic, Source: https://www.anthropic.com/news/releasing-claude-instant-1-2\n",
      "Title : Claude \\ Anthropic, Source: https://www.anthropic.com/claude\n",
      "Title : Claude 2 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2\n",
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n"
     ]
    }
   ],
   "source": [
    "for doc in response[\"context\"]:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58f41cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Do you know about Claude 3?'),\n",
       " AIMessage(content='Yes, I am well aware of Claude 3 AI chatbot from Anthropic. Please provide more context info on how can I help you.'),\n",
       " HumanMessage(content='Tell me about different models in detail.'),\n",
       " AIMessage(content=\"AI: Of course! The Claude 3 model family is a group of three state-of-the-art language models developed by Anthropic. These models are designed to excel across various cognitive tasks and have demonstrated significant advancements over previous models in terms of intelligence, capability, and performance. Here's a detailed overview of each model in the Claude 3 family:\\n\\n1. Claude 3 Haiku: This is the most capable but less intelligent model in the family, with a context window of 200K tokens. It can perform tasks such as task automation, plan and execute complex actions, and interactive coding. While it's not as intelligent or capable as the other two models, Haiku is still an excellent choice for a wide range of applications.\\n2. Claude 3 Sonnet: The second model in the family has a context window of 500K tokens. Sonnet demonstrates higher intelligence and capability than Haiku, with the ability to process longer inputs and provide more accurate responses. It excels in R&D tasks such as research review, brainstorming, and hypothesis generation, as well as financial analysis and forecasting.\\n3. Claude 3 Opus: The most intelligent model in the family has a context window of 1 million tokens. Opus possesses exceptional capabilities, including the ability to navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. It's designed for advanced tasks such as drug discovery, strategy analysis, and natural language classification and sentiment analysis.\\n\\nIn summary, the Claude 3 model family offers a range of capabilities and intelligence levels to suit various applications. While Haiku is suitable for more straightforward tasks, Sonnet and Opus offer enhanced performance and capability for more complex tasks. Please let me know if you have any specific questions or require further assistance.\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.append(HumanMessage(content=\"Tell me about different models in detail.\"))\n",
    "chat_history.append(AIMessage(content=response[\"answer\"]))\n",
    "\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bb2188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI: Of course! Claude 3 Opus is the most intelligent model in the Claude 3 family, designed to excel in advanced cognitive tasks that require a deep understanding of human language and reasoning. With a context window of 1 million tokens, Opus can navigate open-ended prompts and unseen scenarios with remarkable fluency and human-like comprehension.\\n\\nHere are some key features and capabilities of Claude 3 Opus:\\n\\n1. Natural Language Classification and Sentiment Analysis: Opus can accurately classify text into various categories, such as positive/negative sentiment, topic classification, and entity recognition. It can also perform multi-step reasoning to understand the nuances of language and context.\\n2. Drug Discovery and Development: Opus can assist in the discovery of new drugs by analyzing large datasets of chemical compounds, predicting potential side effects, and identifying optimal drug combinations.\\n3. Strategy Analysis and Decision-Making: Opus can analyze complex strategic data, evaluate various scenarios, and provide recommendations for decision-making. This is particularly useful in fields like business, finance, and politics.\\n4. Text Generation and Creativity: Opus can generate coherent and contextually appropriate text, including poetry, stories, and articles. It can also engage in creative writing exercises, such as brainstorming new ideas or continuing a story started by another author.\\n5. Conversational Reasoning and Debate: Opus can participate in natural-sounding conversations, using contextual understanding to respond to questions or engage in debates on various topics.\\n6. Multi-Step Problem Solving: Opus can tackle complex problems by breaking them down into smaller sub-tasks and applying logical reasoning and decision-making. This makes it well-suited for tasks like data analysis, optimization, and machine learning.\\n7. Emotional Understanding and Empathy: Opus has been trained to recognize and respond appropriately to various emotions, allowing it to engage in more nuanced and empathetic conversations with humans.\\n8. Integration with Other Tools and Platforms: Claude 3 Opus can seamlessly integrate with other AI systems, as well as popular productivity and creativity tools like Google Docs or Adobe Creative Suite.\\n\\nOverall, Claude 3 Opus represents the cutting-edge of AI language models, offering unparalleled intelligence and capabilities in a wide range of applications. If you have any specific questions or would like more information on how to use Opus, please feel free to ask!'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Tell me more about Claude 3 Opus.\", \n",
    "                                   \"chat_history\": chat_history})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9783e5a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this video, I explained how to create simple **RAG** chatbot application using **langchain**. Feel free to let me know your views and doubts in comments section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815cbfa-9dc0-4460-8589-f017a80f6d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
