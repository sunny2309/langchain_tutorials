{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a366a5a2",
   "metadata": {},
   "source": [
    "# Simple RAG Application using Langchain\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **Simple LLM App**\n",
    "    * Load LLM\n",
    "    * Create Prompt Teamplate\n",
    "    * Merge Prompt Template & LLM to create Chain\n",
    "2. **Build RAG App**\n",
    "    * Load External Data\n",
    "    * Generate Embeddings\n",
    "    * Build Index in Vector Store (using Embeddings)\n",
    "    * Create Retrieval Chain\n",
    "    \n",
    "    \n",
    "### Installation\n",
    "\n",
    "* **pip install langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26dd51",
   "metadata": {},
   "source": [
    "## 1. Simple LLM App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55160",
   "metadata": {},
   "source": [
    "### 1.1 Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac1f1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\") # base_url = 'http://localhost:11434'\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4153a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm just an AI assistant, I don't have access to information about a person or thing called \"Claude 3.\" Could you please provide more context or details about who or what Claude 3 is? That way, I can try to help you better.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Do you know about Claude 3?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77872fa4",
   "metadata": {},
   "source": [
    "### 1.2 Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ead6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an Artificial Intelligence News Reporter.\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7c84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.3 Merge Prompt Template & LLM to Create LangChain Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204b6bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an Artificial Intelligence News Reporter.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))])\n",
       "| Ollama()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_app = prompt | llm\n",
    "\n",
    "llm_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f997ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ah, a fellow tech enthusiast! *adjusts glasses* Yes, I'm afraid I have some bad news regarding Claude 3. *pauses dramatically*\n",
      "\n",
      "It seems that the highly anticipated release of Claude 3 has been... delayed. *coughs awkwardly* The developers at Brainium have been tight-lipped about the exact reason for the delay, but sources close to the project have revealed that there have been some unexpected challenges in the development process.\n",
      "\n",
      "Apparently, the team has encountered some complexities in integrating the latest AI technologies into the game, and they are working diligently to resolve these issues as soon as possible. *nervous laugh* I know, I know... it's a real bummer.\n",
      "\n",
      "But fear not, my friend! The wait will be worth it, I assure you. Claude 3 is shaping up to be the most innovative and immersive game in the series yet, with groundbreaking AI capabilities that will truly revolutionize the gaming experience. *excitedly*\n",
      "\n",
      "So, keep your eyes peeled for any updates on Claude 3's release date. In the meantime, let's speculate wildly about what could be in store for us... perhaps a new, more realistic AI-powered character model? *winks* Who knows, my friend? The possibilities are endless! *nervous chuckle*\n"
     ]
    }
   ],
   "source": [
    "response = llm_app.invoke({\"query\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298a1b9",
   "metadata": {},
   "source": [
    "## 2. Build RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17678c3a",
   "metadata": {},
   "source": [
    "### 2.1 Load External Data\n",
    "\n",
    "* **pip install beautifulsoup4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0effaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.anthropic.com/news/claude-3-family\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b83771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "        \"https://www.anthropic.com/news/releasing-claude-instant-1-2\",\n",
    "        \"https://www.anthropic.com/news/claude-pro\",\n",
    "        \"https://www.anthropic.com/news/claude-2\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1-prompting\",\n",
    "        \"https://www.anthropic.com/news/claude-3-family\",\n",
    "        \"https://www.anthropic.com/claude\"\n",
    "       ] \n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde55819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ● 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ● 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ● 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca67a5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dddf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings_llm = OllamaEmbeddings(model=\"llama2\") # base_url = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f151bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.5058066844940186,\n",
       " -0.31813421845436096,\n",
       " 2.4193434715270996,\n",
       " 0.2437400221824646,\n",
       " -0.11038058251142502]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_query(\"How are you?\")\n",
    "\n",
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af4cecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 4096)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0467b560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, list, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_documents([\n",
    "                                \"Claude 3 is latest Conversational AI Model from Anthropic.\",\n",
    "                                \"Gemini is latest Conversational AI Model from Google.\",\n",
    "                                \"Llama-2 is latest Conversational AI Model from Meta.\",\n",
    "                                \"Mixtral is latest Conversational AI Model from Mistral AI.\",\n",
    "                                \"GPT-4 is latest Conversational AI Model from OpenAI.\"\n",
    "                               ])\n",
    "\n",
    "len(embeddings), type(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e7e62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3 Build Index in Vector Store\n",
    "\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82b21e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a29c7008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7fa916ec16f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_index = FAISS.from_documents(documents, embeddings_llm)\n",
    "\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f2e20",
   "metadata": {},
   "source": [
    "There are 3 functions to create index.\n",
    "\n",
    "* **from_documents()**\n",
    "* **from_embeddings()**\n",
    "* **from_texts()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae22539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_index.as_retriever()\n",
    "\n",
    "relevant_docs = retriever.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abbfd026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ● 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ● 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ● 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='gets this correct regardless of where the line with the answer sits in the context, with no modification to the prompt format used in the original experiment. As a result, we believe Claude 2.1 is much more reluctant to answer when a sentence seems out of place in a longer context, and is more likely to claim it cannot answer based on the context given. This particular cause of increased reluctance wasn’t captured by evaluations targeted at real-world long context retrieval tasks.Prompting to effectively use the 200K token context windowWhat can users do if Claude is reluctant to respond to a long context retrieval question?\\xa0We’ve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1’s 200K context window.We achieved significantly better results on the same evaluation by adding the sentence “Here is the most relevant sentence in the context:” to the start of Claude’s response. This was enough to raise Claude 2.1’s score from 27% to 98% on the original evaluation.Essentially, by directing the model to look for relevant sentences first, the prompt overrides Claude’s reluctance to answer based on a single sentence, especially one that appears out of place in a longer document.This approach also improves Claude’s performance on single sentence answers that were within context (ie. not out of place). To demonstrate this, the revised prompt achieves 90-95% accuracy when applied to the Yahoo/Viaweb example shared earlier:We’re constantly training Claude to become more calibrated on tasks like this, and we’re grateful to the community for conducting interesting experiments and identifying ways in which we can improve.FootnotesGregory Kamradt, ‘Pressure testing Claude-2.1 200K via Needle-in-a-Haystack’, November 2023RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ● 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ● 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ● 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-2-1-prompting', 'title': 'Long context prompting for Claude 2.1 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content=\"advanced analysis of charts & graphs, financials and market trends, forecastingDifferentiatorHigher intelligence than any other model available.*1M tokens available for specific use cases, please inquire. Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.Cost [Input $/million tokens | Output $/million tokens]$3 | $15Context window200KPotential usesData processing: RAG or search & retrieval over vast amounts of knowledgeSales: product recommendations, forecasting, targeted marketingTime-saving tasks: code generation, quality control, parse text from imagesDifferentiatorMore affordable than other models with similar intelligence; better for scale.Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.Cost [Input $/million tokens | Output $/million tokens]$0.25 | $1.25Context window200KPotential usesCustomer interactions: quick and accurate support in live interactions, translationsContent moderation: catch risky behavior or customer requestsCost-saving tasks: optimized logistics, inventory management, extract knowledge from unstructured dataDifferentiatorSmarter, faster, and more affordable than other models in its intelligence category.Model availabilityOpus and Sonnet are available to use today in our API, which is now generally available, enabling developers to sign up and start using these models immediately. Haiku will be available soon. Sonnet is powering the free experience on claude.ai, with Opus available for Claude Pro subscribers.Sonnet is also available today through Amazon Bedrock and in private preview on Google Cloud’s Vertex AI Model Garden—with Opus and Haiku coming soon to both.Smarter, faster, saferWe do not believe that model intelligence is anywhere near its limits, and we plan to release frequent updates to the Claude 3 model family over the next few months. We're also excited to release a series of features to enhance our models' capabilities, particularly for enterprise use cases and large-scale deployments. These new features will include Tool Use (aka function calling), interactive coding (aka REPL), and more advanced agentic capabilities.As we push the boundaries of AI capabilities, we’re equally committed to ensuring that our safety guardrails keep apace with these leaps in performance. Our hypothesis is that being at the frontier of AI development is the most effective way to steer its trajectory towards positive societal outcomes.We’re excited to see what you create with Claude 3 and hope you will give us feedback to make Claude an even more useful assistant and creative companion. To start building with Claude, visit anthropic.com/claude. FootnotesThis table shows comparisons to models currently available commercially that have released evals. Our model card shows comparisons to models that have been announced but not yet released, such as Gemini 1.5 Pro. In addition, we’d like to note that engineers have worked to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. Source.ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance© 2024 Anthropic PBC\", metadata={'source': 'https://www.anthropic.com/news/claude-3-family', 'title': 'Introducing the next generation of Claude \\\\ Anthropic', 'description': \"Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.\", 'language': 'en'}),\n",
       " Document(page_content='Introducing Claude Pro \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersProductAnnouncementsIntroducing Claude ProSep 7, 2023●1 min readSubscribe todayToday, we’re introducing a paid plan for our Claude.ai\\xa0chat experience, currently available in the US and UK.Since launching in July, users tell us they’ve chosen Claude.ai as their day-to-day AI assistant for its longer context windows, faster outputs, complex reasoning capabilities, and more.\\xa0Many also shared that they would value more file uploads and conversations over longer periods.With Claude Pro, subscribers can now gain 5x more usage of our latest model, Claude 2, for a monthly price of $20 (US) or £18 (UK).This means you can level up your productivity across a range of tasks, including summarizing research papers, querying contracts, and iterating further on coding projects—like this recent demo of building an interactive map.Claude Pro offers:5x more usage than our free tier provides, with the ability to send many more messagesPriority access to Claude.ai during high-traffic periodsEarly access to new features that help you get the most out of ClaudeYou can learn more about these benefits, including how to maximize your usage, here.We’re grateful for your support as we strive to build helpful, honest, and harmless systems that fuel productivity and inspire creativity.RelatedSee AllProductIntroducing the next generation of ClaudeMar 4, 2024 ● 7 min readProductPrompt engineering for business performanceFeb 29, 2024 ● 6 min readProductExpanded legal protections and improvements to our APIDec 19, 2023 ● 1 min readClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyAcceptable Use PolicyResponsible Disclosure PolicyCompliance© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-pro', 'title': 'Introducing Claude Pro \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "371a1143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Releasing Claude Instant 1.2 \\ Anthropic, Source: https://www.anthropic.com/news/releasing-claude-instant-1-2\n",
      "Title : Long context prompting for Claude 2.1 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2-1-prompting\n",
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Introducing Claude Pro \\ Anthropic, Source: https://www.anthropic.com/news/claude-pro\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d38a6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4 Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc931c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I'm familiar with Claude 3. It's the latest conversational AI model developed by Anthropic, a company known for creating advanced language models. Claude 3 is designed to generate more natural and coherent text than its predecessors, and it has shown promising results in various applications such as chatbots, virtual assistants, and content generation.\n",
      "\n",
      "As an AI language model, I have been trained on a large corpus of text data, including texts generated by Claude 3 and other advanced language models. This training allows me to understand and generate text similar to that produced by Claude 3, as well as other conversational AI models. However, it's important to note that my knowledge is based on my training data, and there may be aspects of Claude 3 or other topics that I am not aware of. If you have any specific questions about Claude 3 or any other topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context and your internal knowledge.\n",
    "Give priority to context and if you are not sure then say you are not aware of topic:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "#document_chain  = prompt | llm \n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "response = document_chain.invoke({\n",
    "    \"input\": \"Do you know about Claude 3?\",\n",
    "    \"context\": [Document(page_content=\"Claude 3 is latest Conversational AI Model from Anthropic.\")]\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0625a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fa916ec16f0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based on the provided context and your internal knowledge.\\nGive priority to context and if you are not sure then say you are not aware of topic:\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\\n'))])\n",
       "            | Ollama()\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06676a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b84eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'context', 'answer'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cee072e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I can answer your question about Claude 3. Based on the text you provided, Claude 3 is a family of three state-of-the-art models developed by Anthropic PBC, a research and AI safety company. These models are designed to set new industry benchmarks across a wide range of cognitive tasks and are available for use today.\n",
      "\n",
      "Here are the names and capabilities of each model in the Claude 3 family:\n",
      "\n",
      "1. Claude 3 Haiku: This is the fastest, most compact model in the Claude 3 family, designed for near-instant responsiveness. It answers simple queries and requests with unmatched speed, enabling developers to build seamless AI experiences that mimic human interactions.\n",
      "2. Claude 3 Sonnet: This model strikes the ideal balance between intelligence and speed, particularly for enterprise workloads. It delivers strong performance at a lower cost compared to other models with similar intelligence; better for scale.\n",
      "3. Claude 3 Opus: This is the most advanced and capable model in the family, available to use today in Anthropic's API. It shows a 30% reduction in incorrect answers compared with Claude 2.1 and a 3-4x lower cost-saving task.\n",
      "\n",
      "Anthropic plans to release frequent updates to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. The company is also committed to ensuring that its safety guardrails keep pace with these leaps in performance, as it pushes the boundaries of AI development towards positive societal outcomes.\n",
      "\n",
      "Overall, Claude 3 appears to be a powerful and innovative AI model family designed to set new industry standards across various cognitive tasks.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edcbb0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Claude \\ Anthropic, Source: https://www.anthropic.com/claude\n",
      "Title : Long context prompting for Claude 2.1 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2-1-prompting\n",
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n"
     ]
    }
   ],
   "source": [
    "for doc in response[\"context\"]:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9783e5a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this video, I explained how to create simple **RAG** application using **langchain**. Feel free to let me know your views and doubts in comments section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
